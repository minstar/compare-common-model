{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import queue\n",
    "import pickle\n",
    "import zipfile\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from train_util import *\n",
    "from preprocessing import *\n",
    "from layer import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling Unigram Table\n",
      "Testing Model :cbow, loss : Softmax\n",
      "idx: 200, loss: 2.076458, time: 0.028922, \n",
      "idx: 200, loss: 2.081181, time: 0.026928, \n",
      "idx: 200, loss: 2.077710, time: 0.024936, \n",
      "idx: 200, loss: 2.079751, time: 0.027925, \n",
      "idx: 200, loss: 2.070858, time: 0.025932, \n",
      "idx: 200, loss: 2.072603, time: 0.026927, \n",
      "idx: 200, loss: 2.067445, time: 0.024439, \n",
      "idx: 200, loss: 2.062257, time: 0.033911, \n",
      "idx: 200, loss: 2.055293, time: 0.025950, \n",
      "idx: 200, loss: 2.051205, time: 0.026907, \n",
      "Testing Model :cbow, loss : Negative_Sampling\n",
      "idx: 200, loss: 2.076036, time: 0.024933, \n",
      "idx: 200, loss: 2.074896, time: 0.025931, \n",
      "idx: 200, loss: 2.073688, time: 0.024958, \n",
      "idx: 200, loss: 2.073738, time: 0.025935, \n",
      "idx: 200, loss: 2.079159, time: 0.026899, \n",
      "idx: 200, loss: 2.073700, time: 0.024934, \n",
      "idx: 200, loss: 2.072034, time: 0.025931, \n",
      "idx: 200, loss: 2.072498, time: 0.025928, \n",
      "idx: 200, loss: 2.069589, time: 0.024934, \n",
      "idx: 200, loss: 2.070625, time: 0.028943, \n",
      "Testing Model :cbow, loss : Hierarchical_Softmax\n",
      "idx: 200, loss: 2.078933, time: 0.025911, \n",
      "idx: 200, loss: 2.078874, time: 0.024497, \n",
      "idx: 200, loss: 2.078156, time: 0.028922, \n",
      "idx: 200, loss: 2.067720, time: 0.025905, \n",
      "idx: 200, loss: 2.072543, time: 0.027924, \n",
      "idx: 200, loss: 2.061085, time: 0.025224, \n",
      "idx: 200, loss: 2.022115, time: 0.024933, \n",
      "idx: 200, loss: 2.001003, time: 0.025930, \n",
      "idx: 200, loss: 2.033739, time: 0.023962, \n",
      "idx: 200, loss: 1.971814, time: 0.026332, \n",
      "Testing Model :skipgram, loss : Softmax\n",
      "idx: 200, loss: 2.041392, time: 0.102757, \n",
      "idx: 200, loss: 1.997145, time: 0.098705, \n",
      "idx: 200, loss: 1.961742, time: 0.090759, \n",
      "idx: 200, loss: 1.947597, time: 0.098735, \n",
      "idx: 200, loss: 1.928988, time: 0.106714, \n",
      "idx: 200, loss: 1.921645, time: 0.094748, \n",
      "idx: 200, loss: 1.871427, time: 0.093748, \n",
      "idx: 200, loss: 1.928434, time: 0.102726, \n",
      "idx: 200, loss: 2.050648, time: 0.096396, \n",
      "idx: 200, loss: 2.061095, time: 0.108726, \n",
      "Testing Model :skipgram, loss : Negative_Sampling\n",
      "idx: 200, loss: 2.075080, time: 0.103711, \n",
      "idx: 200, loss: 2.057356, time: 0.101875, \n",
      "idx: 200, loss: 2.034987, time: 0.100130, \n",
      "idx: 200, loss: 2.027050, time: 0.101065, \n",
      "idx: 200, loss: 2.018603, time: 0.098245, \n",
      "idx: 200, loss: 2.010525, time: 0.099735, \n",
      "idx: 200, loss: 2.004137, time: 0.101727, \n",
      "idx: 200, loss: 1.986348, time: 0.101741, \n",
      "idx: 200, loss: 1.992894, time: 0.104707, \n",
      "idx: 200, loss: 1.961709, time: 0.102727, \n",
      "Testing Model :skipgram, loss : Hierarchical_Softmax\n",
      "idx: 200, loss: 1.981783, time: 0.108709, \n",
      "idx: 200, loss: 1.910226, time: 0.098735, \n",
      "idx: 200, loss: 1.979195, time: 0.099734, \n",
      "idx: 200, loss: 2.063710, time: 0.095747, \n",
      "idx: 200, loss: 2.135844, time: 0.108708, \n",
      "idx: 200, loss: 2.307119, time: 0.103724, \n",
      "idx: 200, loss: 2.442610, time: 0.101729, \n",
      "idx: 200, loss: 2.577206, time: 0.099313, \n",
      "idx: 200, loss: 2.578711, time: 0.094352, \n",
      "idx: 200, loss: 2.603864, time: 0.101242, \n"
     ]
    }
   ],
   "source": [
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing the data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "vocab_size = 100000\n",
    "thresholds = 1e-3\n",
    "embedding_size = 200\n",
    "context_size = 5\n",
    "architecture = 'cbow'\n",
    "use_loss = 'Negative_Sampling'\n",
    "sample_size = 25\n",
    "lr = 1e-3\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = preprocess('text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making dictionary :  3.024092197418213\n",
      "unknown count :  4.03930139541626\n",
      "unigram dictionary :  0.03992295265197754\n",
      "subsampling time :  11.861744165420532\n"
     ]
    }
   ],
   "source": [
    "words, word_lst, word_dict, word_reverse_dict, words_index, unigram_dict = total_sampling(file_data=words, subsampling=True, threshold=thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. negative Sampling and Hierarchical Softmax\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling Unigram Table\n"
     ]
    }
   ],
   "source": [
    "sample_table = Unigram_Table(vocab_size,sample_size,unigram_dict)\n",
    "huffman_encode = Huffman_encoding(unigram_dict)\n",
    "huffman_encode.encoding()\n",
    "node_lst, node_dict, reverse_node_dict = huffman_encode.make_node_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "architectures = ['cbow']\n",
    "use_loss = ['Negative_Sampling', 'Hierarchical_Softmax']\n",
    "learning_rate = {'Softmax':1e-5, 'Negative_Sampling':0.05, 'Hierarchical_Softmax':0.07}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Model :cbow, loss : Negative_Sampling\n",
      "idx: 237581, loss: 17.995562, time: 47.821074, \n",
      "idx: 475162, loss: 17.790715, time: 50.704139, \n",
      "idx: 712743, loss: 17.514190, time: 50.414410, \n",
      "idx: 950324, loss: 17.412866, time: 49.125716, \n",
      "idx: 1187905, loss: 17.462446, time: 48.006757, \n",
      "idx: 1425486, loss: 17.451136, time: 47.782228, \n",
      "idx: 1663067, loss: 17.435149, time: 48.809634, \n",
      "idx: 1900648, loss: 17.285121, time: 49.540880, \n",
      "idx: 2138229, loss: 17.302104, time: 49.855042, \n",
      "idx: 2375810, loss: 17.254416, time: 48.343751, \n",
      "idx: 2613391, loss: 17.399910, time: 49.593298, \n",
      "idx: 2850972, loss: 17.311431, time: 49.009414, \n",
      "idx: 3088553, loss: 17.297997, time: 49.372493, \n",
      "idx: 3326134, loss: 17.278456, time: 49.265975, \n",
      "idx: 3563715, loss: 17.393832, time: 49.228044, \n",
      "idx: 3801296, loss: 17.429976, time: 49.352632, \n",
      "idx: 4038877, loss: 17.356720, time: 49.016695, \n",
      "idx: 4276458, loss: 17.397095, time: 49.564100, \n",
      "idx: 4514039, loss: 17.588610, time: 48.927411, \n",
      "idx: 4751620, loss: 17.639009, time: 49.816765, \n",
      "idx: 4989201, loss: 17.450301, time: 49.896779, \n",
      "idx: 5226782, loss: 17.269426, time: 49.506320, \n",
      "idx: 5464363, loss: 17.301922, time: 48.952123, \n",
      "idx: 5701944, loss: 17.405519, time: 49.804773, \n",
      "idx: 5939525, loss: 17.431384, time: 50.149389, \n",
      "idx: 6177106, loss: 17.546080, time: 49.798897, \n",
      "idx: 6414687, loss: 17.602675, time: 50.271293, \n",
      "idx: 6652268, loss: 17.386151, time: 52.227754, \n",
      "idx: 6889849, loss: 17.317443, time: 50.053016, \n",
      "idx: 7127430, loss: 17.391299, time: 48.998759, \n",
      "idx: 7365011, loss: 17.483603, time: 50.694385, \n",
      "idx: 7602592, loss: 17.361335, time: 51.212212, \n",
      "idx: 7840173, loss: 17.354965, time: 50.394098, \n",
      "idx: 8077754, loss: 17.426068, time: 53.017572, \n",
      "idx: 8315335, loss: 17.421682, time: 50.612722, \n",
      "idx: 8552916, loss: 17.398459, time: 50.508874, \n",
      "idx: 8790497, loss: 17.336144, time: 50.460695, \n",
      "idx: 9028078, loss: 17.582931, time: 50.764172, \n",
      "idx: 9265659, loss: 17.366352, time: 50.954220, \n",
      "idx: 9503240, loss: 17.325969, time: 51.600473, \n",
      "idx: 9740821, loss: 17.388143, time: 50.228918, \n",
      "idx: 9978402, loss: 17.437092, time: 51.743442, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a636f97c2d77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0muse_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_table\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                         decay_step=len(words_index)//20, decay_rate=0.98, neg_samples=sample_size, epochs=epochs)\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[1;31m#print (learning_rate[loss_type])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[1;31m#learning_rate['Negative_Sampling'] = learning_rate[loss_type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ysjeong\\Desktop\\Information Retrieval\\assignment\\과제_04\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchitecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cbow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentWord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontextWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ysjeong\\Desktop\\Information Retrieval\\assignment\\과제_04\\model.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, currentWord, contextWords)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchitecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cbow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             loss, grads, grads_idx = CBOW(currentWord, contextWords, inputVectors, outputVectors,\n\u001b[0;32m---> 47\u001b[0;31m                                                 self.use_loss, self.sample_table, self.encoder)\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchitecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'skipgram'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             loss, grads, grads_idx = Skipgram(currentWord, contextWords, inputVectors, outputVectors,\n",
      "\u001b[0;32mC:\\Users\\ysjeong\\Desktop\\Information Retrieval\\assignment\\과제_04\\layer.py\u001b[0m in \u001b[0;36mCBOW\u001b[0;34m(currentWord, contextWords, inputVectors, outputVectors, use_loss, sample_table, encoder)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mneg_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentWord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         loss, grads, grads_idx = negative_sampling(currentWord, contextWords, inputVectors,\n\u001b[0;32m---> 98\u001b[0;31m                                                    hidden, outputVectors, samples=neg_sample)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ysjeong\\Desktop\\Information Retrieval\\assignment\\과제_04\\layer.py\u001b[0m in \u001b[0;36mnegative_sampling\u001b[0;34m(currentWord, contextWord, inputVectors, hiddenVectors, outputVectors, samples)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mtrue_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentWord\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mnega_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mtrue_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhiddenVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for architecture in architectures:\n",
    "    for loss_type in use_loss:\n",
    "        print ('Testing Model :%s, loss : %s' % (architecture, loss_type))\n",
    "        model = Word2Vec(vocab_size, embedding_size, context_size, architecture=architecture, verbose=len(words_index)//50,\n",
    "                        use_loss=loss_type, learning_rate = learning_rate[loss_type], sample_table=sample_table, encoder=node_dict,\n",
    "                        decay_step=len(words_index)//20, decay_rate=0.98, neg_samples=sample_size, epochs=epochs)\n",
    "        model.train(words_index)\n",
    "        #print (learning_rate[loss_type])\n",
    "        #learning_rate['Negative_Sampling'] = learning_rate[loss_type]\n",
    "    # model save\n",
    "np.save('./results/%d_%s_%s_input.npy' % (e, architecture, loss_type), model.params['inputVectors']) # 0.15\n",
    "np.save('./results/%d_%s_%s_output.npy'% (e, architecture, loss_type), model.params['outputVectors']) # 0.15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
