{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import zipfile\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing the data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(file_name):\n",
    "    with zipfile.ZipFile(file_name) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "words = preprocess('text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make word list, dictionary and reverse dictionary\n",
    "def word_dictionary(words):\n",
    "    \n",
    "    ### words : total words list of text8 dataset\n",
    "    word_lst = [['UNK',0]]\n",
    "    word_dict = dict()\n",
    "    \n",
    "    # total 100000 most common (word, frequency) pair\n",
    "    word_lst.extend(Counter(words).most_common(99999)) \n",
    "    \n",
    "    for idx, (word, _) in enumerate(word_lst):\n",
    "        # dictionary : {'word' : 'index'}\n",
    "        word_dict[word] = idx\n",
    "        \n",
    "    # reverse dictionary : {'index' : 'word'}\n",
    "    word_reverse_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "    \n",
    "    return word_lst, word_dict, word_reverse_dict\n",
    "\n",
    "#word_lst, word_dict, word_reverse_dict = word_dictionary(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 기존 dataset에 존재하는 단어에서 상위 10만개 이외의 단어들을 \n",
    "def unknown_count(words, word_lst, word_dict):\n",
    "    words_index = list()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_dict:\n",
    "            idx = word_dict[word]\n",
    "        else:\n",
    "            idx = 0\n",
    "            word_lst[0][1] += 1 # 'UNK' count\n",
    "            \n",
    "        words_index.append(idx)\n",
    "    return words_index, word_lst\n",
    "\n",
    "#words_index, word_lst = unknown_count(words, word_lst, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subsampling function\n",
    "def Subsampling(word_lst, words_index, threshold):\n",
    "    word_prob = dict()\n",
    "    \n",
    "    for idx, (word, freq) in enumerate(word_lst):\n",
    "        prob = freq / len(words_index) # make probability distribution of word\n",
    "        word_prob[idx] = 1 - np.sqrt(threshold / prob)\n",
    "        \n",
    "    return [idx for idx in words_index if word_prob[idx] < 1 - np.random.random()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Unigram_dict(words, word_lst):\n",
    "    unigram = {}\n",
    "    \n",
    "    for idx, (word, freq) in enumerate(word_lst):\n",
    "        unigram[word] = freq / len(words)\n",
    "        \n",
    "    return unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def total_sampling (file_data=words, subsampling=False, threshold=None):\n",
    "    start = time.time()\n",
    "    \n",
    "    word_lst, word_dict, word_reverse_dict = word_dictionary(file_data)   # 10만개 (word, freq) pair, word-index dict and reverse dict\n",
    "    make_dict_time = time.time()\n",
    "    \n",
    "    words_index, word_lst = unknown_count(file_data, word_lst, word_dict) # words_index for subsampling, unknown count\n",
    "    unk_count = time.time()\n",
    "    \n",
    "    if subsampling:\n",
    "        words_index = Subsampling(word_lst, words_index, threshold)\n",
    "        subsample_time = time.time()\n",
    "    \n",
    "    unigram_dict = Unigram_dict(words, word_lst)\n",
    "    unigram_time = time.time()\n",
    "    \n",
    "    print ('making dictionary : ', make_dict_time - start)\n",
    "    print ('unknown count : ', unk_count - make_dict_time)\n",
    "    print ('subsampling time : ', subsample_time - unk_count)\n",
    "    print ('unigram dictionary : ', unigram_time - subsample_time)\n",
    "    \n",
    "    return file_data, word_lst, word_dict, word_reverse_dict, words_index, unigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making dictionary :  2.460446357727051\n",
      "unknown count :  3.2403206825256348\n",
      "subsampling time :  9.886610984802246\n",
      "unigram dictionary :  0.044852495193481445\n"
     ]
    }
   ],
   "source": [
    "words, word_lst, word_dict, word_reverse_dict, words_index, unigram_dict = total_sampling(file_data=words, subsampling=True, threshold=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Make Model in training\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "    # clip input to -10~10\n",
    "    x = np.clip(x, -10, 10)\n",
    "    return (1 / 1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec_test():\n",
    "    dummy_vectors = np.random.randn(100000, 300)\n",
    "    Skipgram('the', ['the', 'anarchism', 'has', 'gone'], inputVectors=dummy_vectors[:50000,:],\n",
    "            outputVectors = dummy_vectors[50000:,:], use_loss='Softmax', tokens=word_dict)\n",
    "\n",
    "    CBOW('the', ['the', 'anarchism', 'has', 'gone'], inputVectors=dummy_vectors[:50000,:],\n",
    "            outputVectors = dummy_vectors[50000:,:], use_loss='Softmax', tokens=word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Skipgram(currentWord, contextWords, inputVectors, outputVectors,\n",
    "             use_loss='Softmax', tokens=None, negative_sampling = None):\n",
    "    \"\"\"\n",
    "    currentWord    : center word String\n",
    "    contextWords   : list of string, which has 2 * C words\n",
    "    inputVectors   : access by row number, such as inputVectors[5]...\n",
    "    outputVectors  : access by row number same as inputVectors\n",
    "    use_loss       : Softmax, Hierarchical Softmax, etc...\n",
    "    tokens         : get index used at inputVectors\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    N, V = inputVectors.shape\n",
    "    C = len(contextWords)\n",
    "    \n",
    "    gradsIn = np.zeros(inputVectors.shape)\n",
    "    gradsOut = np.zeros(outputVectors.shape)\n",
    "    hiddenVectors = np.zeros((1, V))\n",
    "    score = np.zeros((N, 1))\n",
    "    \n",
    "    index = tokens[currentWord]\n",
    "    hiddenVectors = inputVectors[index] # current word to hidden vector\n",
    "    \n",
    "    if use_loss == 'Softmax':\n",
    "        # Softmax forwarding \n",
    "        for i in range(C):\n",
    "            word = contextWords[i]\n",
    "            word_idx = tokens[word]\n",
    "            score[index] += np.dot(hiddenVectors, outputVectors[index].T) # (1, 1)\n",
    "        \n",
    "        exp_out = np.exp(score)\n",
    "        softmax = exp_out / np.sum(exp_out)\n",
    "        dsoftmax = softmax.copy()\n",
    "        # Softmax backwarding\n",
    "        for i in range(C):\n",
    "            word = contextWords[i]\n",
    "            word_idx = tokens[word]\n",
    "            loss = -np.sum(np.log(softmax[word_idx]))\n",
    "            dsoftmax[word_idx] -= 1\n",
    "            \n",
    "        loss /= N\n",
    "        #print (dsoftmax.shape, hiddenVectors.shape)\n",
    "        gradsOut = np.dot(dsoftmax, hiddenVectors.reshape(1,-1))\n",
    "        gradshidden = np.dot(dsoftmax.T, outputVectors)\n",
    "        gradsIn[index] = gradshidden\n",
    "        \n",
    "        # return loss, gradsIn, gradsOut\n",
    "        \n",
    "    #elif use_loss == 'Hierarchical_Softmax':\n",
    "        \n",
    "    elif use_loss == 'Negative_Sampling':\n",
    "        # get unigram distribution and negative sampling index\n",
    "        negative_sample_idx = negative_sampling.sampling(index)\n",
    "        # negative sampling model forwarding\n",
    "    return loss, gradsIn, gradsOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CBOW(currentWord, contextWords, inputVectors, outputVectors,\n",
    "             use_loss='Softmax', tokens=None, negative_sampling=None):\n",
    "    \"\"\"\n",
    "    currentWord    : center word String\n",
    "    contextWords   : list of string, which has 2 * C words\n",
    "    inputVectors   : access by row number, such as inputVectors[5]...\n",
    "    outputVectors  : access by row number same as inputVectors\n",
    "    use_loss       : Softmax, Hierarchical Softmax, etc...\n",
    "    tokens         : get index used at inputVectors\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    N, V = inputVectors.shape\n",
    "    C = len(contextWords)\n",
    "    \n",
    "    gradsIn = np.zeros(inputVectors.shape)\n",
    "    gradsOut = np.zeros(outputVectors.shape)\n",
    "    hiddenVectors = np.zeros((1, V))\n",
    "    score = np.zeros((N, 1))\n",
    "    \n",
    "    index = tokens[currentWord]\n",
    "    \n",
    "    if use_loss == 'Softmax':\n",
    "        # Softmax forwarding\n",
    "        for i in range(C):\n",
    "            word = contextWords[i]\n",
    "            word_idx = tokens[word]\n",
    "            hiddenVectors += inputVectors[word_idx]\n",
    "        \n",
    "        score = np.dot(outputVectors, hiddenVectors.T)\n",
    "        exp_score = np.exp(score)\n",
    "        softmax = exp_score / np.sum(exp_score)\n",
    "        dsoftmax = softmax.copy()\n",
    "        \n",
    "        # Softmax backwarding\n",
    "        loss = -np.sum(np.log(softmax[index]))\n",
    "        loss /= N\n",
    "        \n",
    "        dsoftmax[index] -= 1\n",
    "        gradsOut = np.dot(dsoftmax, hiddenVectors)\n",
    "        gradshidden = np.dot(dsoftmax.T, outputVectors)\n",
    "        \n",
    "        for i in range(C):\n",
    "            word = contextWords[i]\n",
    "            word_idx = tokens[word]\n",
    "            gradsIn[word_idx] = gradshidden\n",
    "\n",
    "        # return loss, gradsIn, gradsOut\n",
    "        \n",
    "    #elif use_loss == 'Hierarchical_Softmax':\n",
    "        \n",
    "    elif use_loss == 'Negative_Sampling':\n",
    "        # get unigram distribution and negative sampling index\n",
    "        negative_sample_idx = negative_sampling.sampling(index)\n",
    "        # negative sampling model forwarding\n",
    "        \n",
    "    return loss, gradsIn, gradsOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make Negative Sampling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unigram_Table:\n",
    "    #--- 3/4 power of unigram distribution selected by mikolov et al. 2013\n",
    "    #--- make table and recall with index\n",
    "    \"\"\"\n",
    "    A list of indices of tokens in the vocab following a power law distribution,\n",
    "    used to draw negative samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, count, unigram_dictionary=dict()):\n",
    "        self.count = count\n",
    "        vocab_size = len(vocab)\n",
    "        power = 0.75\n",
    "        table_size = int(1e8) # unigram table length\n",
    "        table = np.zeros(table_size, np.uint32)\n",
    "        \n",
    "        norm_val = sum(math.pow(uni_prob, power) for uni_prob in unigram_dictionary.values())\n",
    "        \n",
    "        print ('Filling Unigram Table')\n",
    "        p = 0 # Cumulative Probability\n",
    "        i = 0\n",
    "        \n",
    "        for idx, (word, prob) in enumerate(unigram_dictionary.items()):\n",
    "            p += float(math.pow(prob, power)) / norm_val\n",
    "            while (i < table_size) and (float(i) / table_size < p):\n",
    "                table[i] = idx\n",
    "                i += 1\n",
    "                \n",
    "        self.table = table\n",
    "        \n",
    "    def sampling(self, true_idx):\n",
    "        \"\"\"\n",
    "        true_idx : index of target word\n",
    "        return : except target word, count number of whole words\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            indices = np.random.randint(low=0, high=len(self.table), size=self.count)\n",
    "            if true_idx not in indices:\n",
    "                break\n",
    "            \n",
    "        return [self.table[i] for i in indices]\n",
    "    \n",
    "table = Unigram_Table(words, count=5, unigram_dictionary=unigram_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling Unigram Table\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def low_freq_pair(freq_dict):\n",
    "    \"\"\"\n",
    "    freq_dict : (word, word_frequency) pair of dictionary\n",
    "    \"\"\"\n",
    "    low_prob_word = sorted(freq_dict.items(), key=lambda x : x[1])\n",
    "    \n",
    "    return low_prob_word[0][0], low_prob_word[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### huffman code ###\n",
    "def huffman_coding(freq_dict):\n",
    "    \"\"\"\n",
    "    freq_dict : (word, word_frequency) pair of dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # when 2 word left, it gives the 0 and 1 binary equally\n",
    "    if (len(freq_dict) == 2):\n",
    "        return dict(zip(freq_dict.keys(), ['0', '1']))\n",
    "    \n",
    "    # making all frequency as tree\n",
    "    new_freq_dict = freq_dict.copy()\n",
    "    word_1, word_2 = low_freq_pair(freq_dict) # 2 word which has low frequency\n",
    "    freq_1, freq_2 = new_freq_dict.pop(word_1), new_freq_dict.pop(word_2) # frequency of each word\n",
    "    new_freq_dict[word_1 + word_2] = freq_1 + freq_2\n",
    "    \n",
    "    # recursive function\n",
    "    new_dict = huffman_coding(new_freq_dict)\n",
    "    add_freq = new_dict.pop(word_1 + word_2)\n",
    "    new_dict[word_1], new_dict[word_2] = add_freq + '0', add_freq + '1'\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
