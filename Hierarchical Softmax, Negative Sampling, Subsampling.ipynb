{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angel\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import queue\n",
    "import pickle\n",
    "import zipfile\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_util import *\n",
    "from preprocessing import *\n",
    "from layer import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling Unigram Table\n",
      "Testing Model :skipgram, loss : Negative_Sampling\n",
      "idx: 200, loss: 2.052836, time: 0.080785, \n",
      "idx: 200, loss: 1.903186, time: 0.065862, \n",
      "idx: 200, loss: 1.669679, time: 0.074799, \n",
      "idx: 200, loss: 1.480582, time: 0.063822, \n",
      "idx: 200, loss: 1.454608, time: 0.062802, \n",
      "idx: 200, loss: 1.470768, time: 0.061833, \n",
      "idx: 200, loss: 1.480899, time: 0.071850, \n",
      "idx: 200, loss: 1.412037, time: 0.063824, \n",
      "idx: 200, loss: 1.470574, time: 0.065792, \n",
      "idx: 200, loss: 1.453964, time: 0.071805, \n",
      "Testing Model :skipgram, loss : Softmax\n",
      "idx: 200, loss: 2.078240, time: 0.093749, \n",
      "idx: 200, loss: 2.046164, time: 0.074830, \n",
      "idx: 200, loss: 1.987724, time: 0.064831, \n",
      "idx: 200, loss: 1.920189, time: 0.065791, \n",
      "idx: 200, loss: 1.859566, time: 0.078820, \n",
      "idx: 200, loss: 1.709318, time: 0.074806, \n",
      "idx: 200, loss: 1.630252, time: 0.083766, \n",
      "idx: 200, loss: 1.575712, time: 0.067785, \n",
      "idx: 200, loss: 1.502536, time: 0.080783, \n",
      "idx: 200, loss: 1.478899, time: 0.087796, \n",
      "Testing Model :skipgram, loss : Hierarchical_Softmax\n",
      "idx: 200, loss: 2.048567, time: 0.072774, \n",
      "idx: 200, loss: 1.949226, time: 0.066823, \n",
      "idx: 200, loss: 1.676783, time: 0.074801, \n",
      "idx: 200, loss: 1.533248, time: 0.087764, \n",
      "idx: 200, loss: 1.462073, time: 0.072835, \n",
      "idx: 200, loss: 1.461995, time: 0.069813, \n",
      "idx: 200, loss: 1.397156, time: 0.071778, \n",
      "idx: 200, loss: 1.414898, time: 0.077793, \n",
      "idx: 200, loss: 1.440722, time: 0.069841, \n",
      "idx: 200, loss: 1.388024, time: 0.067789, \n",
      "Testing Model :cbow, loss : Negative_Sampling\n",
      "idx: 200, loss: 2.054725, time: 0.028949, \n",
      "idx: 200, loss: 1.996224, time: 0.020915, \n",
      "idx: 200, loss: 1.900761, time: 0.022939, \n",
      "idx: 200, loss: 1.763607, time: 0.020942, \n",
      "idx: 200, loss: 1.644815, time: 0.021989, \n",
      "idx: 200, loss: 1.592908, time: 0.022928, \n",
      "idx: 200, loss: 1.514107, time: 0.020948, \n",
      "idx: 200, loss: 1.495608, time: 0.021901, \n",
      "idx: 200, loss: 1.421661, time: 0.032943, \n",
      "idx: 200, loss: 1.382992, time: 0.022938, \n",
      "Testing Model :cbow, loss : Softmax\n",
      "idx: 200, loss: 2.066192, time: 0.021948, \n",
      "idx: 200, loss: 2.049217, time: 0.020937, \n",
      "idx: 200, loss: 2.031941, time: 0.021911, \n",
      "idx: 200, loss: 1.994668, time: 0.020972, \n",
      "idx: 200, loss: 1.971139, time: 0.022943, \n",
      "idx: 200, loss: 1.940052, time: 0.023931, \n",
      "idx: 200, loss: 1.925288, time: 0.019947, \n",
      "idx: 200, loss: 1.859514, time: 0.058812, \n",
      "idx: 200, loss: 1.822110, time: 0.023938, \n",
      "idx: 200, loss: 1.791263, time: 0.026966, \n",
      "Testing Model :cbow, loss : Hierarchical_Softmax\n",
      "idx: 200, loss: 2.058313, time: 0.028913, \n",
      "idx: 200, loss: 2.019075, time: 0.024903, \n",
      "idx: 200, loss: 1.942614, time: 0.023936, \n",
      "idx: 200, loss: 1.838523, time: 0.025933, \n",
      "idx: 200, loss: 1.729765, time: 0.023973, \n",
      "idx: 200, loss: 1.598397, time: 0.036862, \n",
      "idx: 200, loss: 1.544399, time: 0.024935, \n",
      "idx: 200, loss: 1.488923, time: 0.023973, \n",
      "idx: 200, loss: 1.468323, time: 0.019910, \n",
      "idx: 200, loss: 1.467862, time: 0.020975, \n"
     ]
    }
   ],
   "source": [
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing the data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to config file\n",
    "#parameters\n",
    "vocab_size = 100000\n",
    "thresholds = 1e-3\n",
    "embedding_size = 200\n",
    "context_size = 5\n",
    "architecture = 'cbow'\n",
    "use_loss = 'Negative_Sampling'\n",
    "sample_size = 25\n",
    "lr = 1e-3\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = preprocess('text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making dictionary :  3.089906692504883\n",
      "unknown count :  4.180799961090088\n",
      "unigram dictionary :  0.05385613441467285\n",
      "subsampling time :  10.712352275848389\n"
     ]
    }
   ],
   "source": [
    "words, word_lst, word_dict, word_reverse_dict, words_index, unigram_dict = total_sampling(file_data=words, subsampling=True, threshold=thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.06241594118789615\n"
     ]
    }
   ],
   "source": [
    "print (word_reverse_dict[1], unigram_dict[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. negative Sampling and Hierarchical Softmax\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling Unigram Table\n"
     ]
    }
   ],
   "source": [
    "sample_table = Unigram_Table(vocab_size,sample_size,unigram_dict)\n",
    "huffman_encode = Huffman_encoding(unigram_dict)\n",
    "huffman_encode.encoding()\n",
    "node_lst, node_dict, reverse_node_dict = huffman_encode.make_node_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = ['cbow', 'skipgram']\n",
    "use_loss = ['Negative_Sampling', 'Hierarchical_Softmax']\n",
    "learning_rate = {'Softmax':1e-5, 'Negative_Sampling':0.025, 'Hierarchical_Softmax':0.025}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Model :cbow, loss : Negative_Sampling\n",
      "idx: 237507, loss: 5.692648, time: 68.845177, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0a368cae1b50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                         \u001b[0muse_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_table\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                         decay_step=len(words_index)//20, decay_rate=0.98, neg_samples=sample_size, epochs=epochs)\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m#print (learning_rate[loss_type])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#learning_rate['Negative_Sampling'] = learning_rate[loss_type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\4_2\\Information Retrieval\\assignment\\과제_04\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchitecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cbow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentWord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontextWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\4_2\\Information Retrieval\\assignment\\과제_04\\model.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, currentWord, contextWords)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchitecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cbow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             loss, grads, grads_idx = CBOW(currentWord, contextWords, inputVectors, outputVectors,\n\u001b[1;32m---> 47\u001b[1;33m                                                 self.use_loss, self.sample_table, self.encoder)\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchitecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'skipgram'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             loss, grads, grads_idx = Skipgram(currentWord, contextWords, inputVectors, outputVectors,\n",
      "\u001b[1;32m~\\Desktop\\4_2\\Information Retrieval\\assignment\\과제_04\\layer.py\u001b[0m in \u001b[0;36mCBOW\u001b[1;34m(currentWord, contextWords, inputVectors, outputVectors, use_loss, sample_table, encoder)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mneg_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentWord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         loss, grads, grads_idx = negative_sampling(currentWord, contextWords, inputVectors,\n\u001b[1;32m---> 98\u001b[1;33m                                                    hidden, outputVectors, samples=neg_sample)\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\4_2\\Information Retrieval\\assignment\\과제_04\\layer.py\u001b[0m in \u001b[0;36mnegative_sampling\u001b[1;34m(target, input_word, inputVectors, hiddenVectors, outputVectors, samples)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mtrue_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mnega_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mtrue_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddenVectors\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# (1, 6) (6, )\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for architecture in architectures:\n",
    "    for loss_type in use_loss:\n",
    "        print ('Testing Model :%s, loss : %s' % (architecture, loss_type))\n",
    "        model = Word2Vec(vocab_size, embedding_size, context_size, architecture=architecture, verbose=len(words_index)//50,\n",
    "                        use_loss=loss_type, learning_rate = learning_rate[loss_type], sample_table=sample_table, encoder=node_dict,\n",
    "                        decay_step=len(words_index)//20, decay_rate=0.98, neg_samples=sample_size, epochs=epochs)\n",
    "        model.train(words_index)\n",
    "        #print (learning_rate[loss_type])\n",
    "        #learning_rate['Negative_Sampling'] = learning_rate[loss_type]\n",
    "    # model save\n",
    "np.save('./results/%d_%s_%s_input.npy' % (e, architecture, loss_type), model.params['inputVectors']) # 0.15\n",
    "np.save('./results/%d_%s_%s_output.npy'% (e, architecture, loss_type), model.params['outputVectors']) # 0.15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
